我们重点提出了四项有前景的研究方向，以提升数据中心大语言模型（LLM）推理的性能：**高带宽闪存**（High Bandwidth Flash），可提供接近HBM带宽、但容量达其10倍的存储能力；**近存计算**（Processing-Near-Memory）与**3D内存-逻辑堆叠**（3D memory-logic stacking），以实现高内存带宽；以及**低延迟互连**（low-latency interconnect），用于加速通信。此外，我们还回顾了这些技术在移动设备上的适用性。
# 介绍
当一位作者于1976年开启其职业生涯时，计算机体系结构会议中约40%的论文来自工业界；至2025年国际计算机体系结构研讨会（ISCA），该比例已降至4%以下，反映出研究与实践之间近乎脱节。为重建二者的历史纽带，我们提出若干研究方向——若得以推进，将有助于应对AI产业当前面临的一些最严峻的硬件挑战。

大语言模型（LLM）推理正陷入一场危机。硬件的快速演进推动了AI的进步，预计未来5–8年内推理芯片的年销售额将增长4至6倍。尽管训练展现了显著的AI突破，但推理成本才最终决定其经济可行性；随着模型使用量激增，企业发现部署最先进模型的成本极为高昂。

新兴趋势进一步加剧了推理难度：
- **专家混合（MoE）** ：不同于单一密集型前馈块，MoE采用数十至数百个专家（如DeepSeek-v3启用256个专家），按需选择性调用。这种稀疏性使模型规模可大幅扩展以提升质量，同时仅小幅增加训练成本；但在推理阶段，MoE却因内存与通信需求剧增而加重负担。
- **推理模型（Reasoning models）** ：采用“先思考、后行动”策略以提升质量——在生成最终答案前，先生成一长串“思考”token，类似人类逐步解题。该过程显著增加生成延迟，且长序列的“思考”token对内存构成巨大压力。
- **多模态**：LLM从文本生成拓展至图像、音频与视频生成，更大尺寸的数据类型远超纯文本生成的需求。
- **长上下文窗口**：上下文窗口指LLM生成答案时所能观测的信息量；更长上下文有助于提升质量，但亦同步推高计算与内存开销。
- **检索增强生成（RAG）** ：通过访问用户专属知识库获取相关信息作为额外上下文，以提升LLM输出质量，从而增加资源消耗。
- **扩散模型（Diffusion）** ：不同于自回归方法逐token生成，扩散模型一次性生成全部token（如整张图像），再经迭代去噪达到目标质量；与其他趋势不同，扩散模型主要增加计算需求，而非内存或通信开销。
# 当前大模型推理硬件及其低效性
我们首先回顾大语言模型（LLM）推理的基本原理及其在主流AI架构中的主要瓶颈，重点聚焦于数据中心场景下的LLM。移动设备上的LLM受制于不同约束，因此适用方案亦异（例如，HBM不可行）。
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201114824.png)
以Transformer为核心的LLM包含两个特征迥异的推理阶段：**Prefill（预填充）** 与 **Decode（解码）** （见图1）。Prefill类似于训练过程，即并行处理整个输入序列的所有token，因而本质上是计算密集型；而Decode则本质上是顺序执行的——每一步仅生成一个输出token（“自回归”方式），故为内存受限型。**键值缓存**（KV Cache）连接这两个阶段，其大小正比于输入与输出序列总长度。尽管图1中二者并列呈现，Prefill与Decode实际关联较弱，常运行于不同服务器上。解耦式推理（Disaggregated inference）支持批处理等软件优化，可缓解Decode的内存瓶颈。

GPU与谷歌的张量处理单元（TPU）是数据中心中广泛用于训练与推理的加速器。历史上，推理专用版本通常由训练系统缩减而来（如芯片数量更少、单芯片规模更小或内存/性能更低）。迄今尚无专为LLM推理设计的GPU/TPU。由于Prefill与训练相似，而Decode显著不同，现有硬件在Decode阶段面临两大挑战：

**Decode挑战 1：内存瓶颈**  
自回归式Decode使推理天然受限于内存带宽，而新兴软件趋势进一步加剧该问题；相比之下，硬件发展路径却背道而驰。
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201115346.png)
**AI处理器遭遇“内存墙”** ：当前数据中心GPU/TPU依赖**高带宽内存**（HBM），将多个HBM堆栈连接至单一单片式专用集成电路（ASIC）加速器（参见图2与表1）。然而，内存带宽提升速度远落后于计算能力（浮点运算每秒次数，FLOPS）。例如，NVIDIA GPU的64位FLOPS从2012至2022年提升了80倍，而带宽仅增长17倍。该差距将持续扩大。
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201120015.png)
**HBM成本持续攀升**：以单个HBM堆栈为例，其单位容量成本（/GB）与带宽成本（/Gbps）在2023–2025年间均上涨了1.35倍——这一涨幅源于制造与封装难度随HBM堆栈及动态随机存取存储器（DRAM）密度提升而同步增加。相较之下，图3(b)显示标准双倍数据速率（DDR4）DRAM的等效成本呈下降趋势：2022–2025年间，容量成本降至0.54×，带宽成本降至0.45×。然而，受意外需求驱动，2026年所有内存与存储设备价格全面飙升；我们预计，HBM与DRAM长期存在分化的价格走势将持续。

**DRAM密度增长正在放缓**。对单颗DRAM芯片而言，规模效应亦日益乏力：2014年首发的8 Gbit DRAM芯片，其四倍容量提升耗时将超过10年（此前每3–6年即可实现四倍增长）。

**仅依赖SRAM的方案不足以应对挑战**。Cerebras与Groq曾尝试采用满版晶圆级芯片（full reticle chips），以片上SRAM替代DRAM与HBM，以规避相关瓶颈（Cerebras甚至采用了晶圆级集成技术）。尽管这些方案在公司创立初期看似可行，但随着大语言模型（LLMs）迅速压倒片上SRAM容量上限，二者最终均不得不转向外部DRAM。

**解码挑战2：端到端延迟**
**面向用户的应用要求低延迟**。与耗时数周的训练不同，推理需响应实时请求，通常需在数秒甚至更短时间内完成。低延迟对面向用户型推理至关重要（批处理或离线推理则无此要求）。根据应用场景，延迟可定义为**全部输出token生成完成所需时间**（time-to-completion），或**首个用户可见token生成所需时间**（time-to-first-token）——二者均面临严峻挑战：
- **生成完成时间挑战**：Decode逐token生成，输出越长，总延迟越高；长输出序列自然拉长延迟，而长输入序列同样会拖慢整体速度，因其在Decode与Prefill阶段均需访问KV Cache，耗时更多。
- **首token生成时间挑战**：长输入序列与RAG（检索增强生成）显著增加生成前的计算量，从而推高首token延迟；推理模型（Reasoning models）亦加剧该问题——其在生成首个用户可见token前，需先生成大量“思考”token。

**片间互连延迟压倒带宽瓶颈**  LLM出现前，数据中心推理通常运行于单芯片上，而训练需依赖超算；超算互连设计以带宽优先、容忍较高延迟。LLM推理彻底改变了这一格局：
- 由于模型权重庞大，LLM推理现需多芯片系统，软件分片机制导致频繁通信；MoE与长序列模型进一步扩大系统规模，以满足更大内存容量需求。
- 与训练不同，Decode阶段批量规模小，网络消息通常较短；此时，**低延迟比高带宽更为关键**——在大型网络中，高频次的小消息传输受制于延迟而非带宽。
表2总结了Decode推理的主要硬件瓶颈及对应研究方向。除扩散模型（Diffusion）外，其余趋势均加剧内存与互连延迟压力（扩散模型仅提升计算需求，相对易于满足）。因此，本文聚焦于内存与互连延迟的优化路径，而非计算能力提升。后文将详述四大有前景的研究方向，以应对上述挑战。
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201121311.png)
# 四个研究机会重新思考LLM推理的硬件实现
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201122718.png)
性能/成本指标用于衡量AI系统的效率。现代指标——强调真实场景下的性能、**总拥有成本**（TCO）、平均功耗及**二氧化碳当量排放**（CO₂e）——为系统设计设定了新目标
- **性能必须具备实际意义**：对LLM Decode推理而言，巨型芯片上的高FLOPS并不必然代表高性能；相反，我们需要高效扩展内存带宽与容量，并优化互连速度。
- **性能必须在数据中心容量约束下交付**，通常受限于功耗、空间与CO₂e预算。
- **功耗与CO₂e是一阶优化目标**：功耗直接影响TCO与数据中心承载能力；运行功耗及其能源清洁度决定运营碳排放；制造良率与产品生命周期决定隐含碳排放。
接下来，我们阐述四项有前景的研究方向，以应对Decode阶段的核心挑战（详见表2底部）。尽管各自独立描述，但它们具有协同效应；一个架构可有效融合多项技术。所有方向均能提升 **性能/TCO**、**性能/CO₂e** 与 **性能/功耗** 比值。

![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201123402.png)
**① 高带宽闪存实现10倍容量提升**
高带宽闪存（High Bandwidth Flash, **HBF**）通过类似HBM的方式堆叠闪存晶粒，将HBM的带宽优势与闪存的大容量特性相结合（见图4(a)）。HBF可使单节点内存容量提升10倍，从而缩小系统规模，降低功耗、总拥有成本（TCO）、碳排放（CO₂e）及网络开销。表3对比了HBF与HBM、DDR及低功耗DDR（LPDDR）DRAM。替代方案的短板在于：DDR5带宽不足、HBM容量受限、而HBF则存在写入耐久性限制与高读取延迟问题。

HBF另一显著优势是可持续扩容能力：闪存容量仍保持约每三年翻倍，而如前所述，DRAM密度增长已明显放缓。
需应对两项广为人知的闪存限制：
- **有限写入耐久性**（Limited write endurance）：擦写循环会磨损闪存单元。因此，HBF必须仅存储更新频率极低的数据，例如推理时的模型权重或缓慢变化的上下文。
- **页级读取且延迟高**（Page-based reads with high latency）：闪存以页为单位读取（典型页大小为10 KB量级），其延迟远高于DRAM（微秒级 vs 纳秒级）。小数据量读取会显著降低有效带宽。
上述问题意味着HBF无法完全取代HBM；系统仍需常规DRAM来处理不适合存入HBF的数据。

高带宽闪存（HBF）为大语言模型推理带来的全新能力：
- **10倍权重内存**：推理阶段模型权重固定不变，HBF的10倍扩容能力可承载海量权重——例如巨型混合专家模型（MoE）——使训练远超当前成本阈值的超大规模模型成为可能。
- **10倍上下文内存**：受限于闪存的写入耐久性，HBF不适合存储随每轮查询或token生成实时更新的KV缓存数据，但可完美承载慢变化上下文：
    - 大语言模型搜索引擎所用的网络语料库，存储数十亿级互联网文档；
    - AI代码生成工具所用的代码数据库：存储数十亿行代码；
    - AI学术助手所用的论文语料库：追踪数百万篇研究论文。
- **缩小推理系统规模**：内存容量决定了运行模型所需的最小硬件配置，HBF可大幅精简系统架构，优化通信效率、可靠性与资源分配。
- **提升资源承载能力**：HBF可降低对HBM独占式架构的依赖，缓解全球范围内主流内存芯片的供应短缺问题。

HBF催生的前沿研究问题：
- 如何通过软件层面的设计，适配闪存有限的写入耐久性与页级读取高延迟特性？
- 系统中传统内存与HBF的最优配比应为多少？
- 是否可从技术层面突破HBF自身的硬件限制？
- 面向移动端与数据中心的HBF，如何设计差异化配置方案？

**② 近存计算（Processing-Near-Memory）实现高带宽**
数十年前便已提出的**存内计算（Processing-in-Memory, PIM）**，通过在存储芯片的内存 bank 上搭载小型低功耗处理器来获取高带宽。尽管PIM能提供惊人的带宽，但也面临两大关键挑战：软件分片与存算耦合。前者限制了可在 PIM 上高效运行的软件核数量，后者则会降低计算逻辑的功耗与面积效率。

与之相对，**近存计算（Processing-Near-Memory, PNM）** 是一种将内存与计算逻辑物理靠近部署、但仍采用独立芯片的技术。PNM的一种实现形式是 3D计算-逻辑堆叠技术（详见后面的③）。

遗憾的是，近期部分论文模糊了PIM与PNM的界限——无论计算逻辑是否直接嵌入内存芯片，均统一以“PIM”指代。本文采用清晰明确的划分标准：
- PIM：处理器与内存集成在**同一芯片**上的设计
- PNM：处理器与内存物理接近但分属**独立芯片**的设计  
这一界定让两者的概念不再混淆。
![image.png](https://raw.githubusercontent.com/waibibab-cs/blog_img/main/cdnimg/20260201125854.png)
如果硬件的软件适配难度过大，其自身优势便毫无意义——这正是我们在存内计算（PIM）与数据中心大语言模型（LLM）实践中得到的经验。表4列举了为何**近存计算（PNM）更适配LLM推理场景**（尽管PNM在带宽与功耗层面存在短板）：  
具体而言，PIM要求软件将LLM的内存结构切分为大量互不交互的小分片，以适配32-64MB规格的内存bank；而PNM支持的分片尺寸可达PIM的1000倍，能大幅降低LLM模型切分的难度与通信开销。此外，DRAM工艺节点的功耗与散热预算极其有限，PIM的计算能力能否满足需求也存疑。

不过，上述结论仅针对数据中心LLM；在移动设备中，二者的优劣对比并不明确：移动设备受能耗约束更强，且运行的LLM通常参数量更少、上下文长度更短、数据类型更小，再加上单用户场景决定了batch size极低。这些特性不仅简化了模型分片的需求，还降低了计算量与散热压力，使得PIM的短板不再突出，因此PIM在移动设备中具备可行的落地空间。

**③ 3D存算堆叠技术实现高带宽**
与2D硬件（内存I/O位于芯片边缘）不同，3D堆叠（见图4(b)）通过硅通孔（Through Silicon Vias, **TSVs**）实现垂直互连，构建了宽而密集的内存接口，能以低功耗提供高带宽。

3D存算堆叠分为两种实现方案：
1. **基于HBM基底芯片的存算方案（Compute-on-HBM-base-die）**  
    复用HBM现有设计，将计算逻辑嵌入HBM基底芯片。由于内存接口保持不变，带宽与传统HBM持平，但数据传输路径缩短，功耗可降低2-3倍。
2. **定制化3D方案**  
    通过更宽、更密集的内存接口和更先进的封装技术，可实现比HBM复用方案更高的带宽与带宽功耗比。

尽管3D堆叠在带宽与功耗上表现更优，仍面临几大挑战：
1. **散热问题（Thermal）**  
    3D结构的散热难度远高于2D芯片，原因是其有效散热表面积更小。一种解决方案是限制计算逻辑的FLOPS（每秒浮点运算次数）：通过降低时钟频率和电压运行——而LLM推理阶段本身算术强度较低，恰好适配这种运行模式。
2. **存算耦合问题（Memory-logic coupling）**  
    3D存算堆叠的内存接口可能需要统一的行业标准来规范。

3D堆叠技术带来的前沿研究方向：
- 3D堆叠架构的内存带宽与计算FLOPS的比例，与现有系统差异显著，软件该如何适配？
- 面对多类型内存的系统，如何高效映射大语言模型？
- 若有必要，3D存算堆叠模块之间、以及与主AI处理器之间该如何实现通信？
- 不同设计选择下的带宽、功耗、散热、可靠性之间有哪些权衡？比如：计算芯片位于堆叠上层还是下层？每个堆叠单元集成多少个内存芯片？
- 面向移动设备与数据中心大语言模型加速卡的方案，这些技术机遇有哪些差异？

④ 低延迟互连技术
前文介绍的①-③技术可同时优化延迟与吞吐量：更高的内存带宽能缩短每轮Decode迭代的延迟，而单加速芯片内存容量的提升可缩小系统规模，降低通信开销。面向数据中心的另一潜在低延迟优化方向，是重新思考网络延迟与带宽的权衡——推理场景对互连延迟更为敏感。具体方向包括：
- **高连通性拓扑结构**：高连通性的网络拓扑（如树形、蜻蜓形、高维环面形）可减少数据传输跳数，进而降低延迟。这类拓扑可能会损失部分带宽，但能显著优化延迟表现。
- **网络内计算（Processing-in-network）** ：大语言模型常用的通信集合操作（包括广播、全归约、MoE调度与收集等），非常适合通过网络内计算加速，可同时优化带宽与延迟。例如，树形拓扑结合网络内聚合功能，可实现低延迟、高吞吐量的全归约操作。
- **AI芯片优化**：延迟指标会从多方面影响芯片设计，潜在优化方向如下：
    - 将小型分组数据包直接存入片上SRAM，而非片外DRAM；
    - 把计算引擎布局在靠近网络接口的位置，缩短数据传输耗时。
- **可靠性设计**：协同设计可靠性与互连系统可实现双重收益：
    - 设置本地备用节点，可减少系统故障，以及故障节点任务迁移至其他健康节点时带来的延迟与吞吐量损失；
    - 若大语言模型推理不要求通信完全精准，可在消息超时后，使用伪造数据或历史结果继续流程，无需等待滞后消息，在降低延迟的同时仍能保证结果质量满足需求。
# 相关工作
**高带宽闪存（High Bandwidth Flash, HBF）**  
闪迪（SanDisk）率先提出了 HBF，即一种类HBM架构的闪存方案，旨在突破其带宽限制（SK Hynix 后续也加入了该研发）。微软研究人员提出了一种新型 AI 推理专用内存架构，聚焦于提升读取性能、增加存储密度，并兼顾写入性能与数据保留时间。虽然未明确提及，HBF 正是所提出的新型 AI 内存的一个具体实例。另一篇研究论文则提出将闪存集成到移动处理器中，用于设备端 LLM 推理；其中采用 LPDDR 接口以满足 Prefill 阶段带宽需求较低的特点，并通过 Processing-Near-Flash 技术应对 Decode 阶段对高带宽的需求。

**近存计算（Processing-Near-Memory）**  
3D计算逻辑堆叠技术日益受到关注，被视为实现高于HBM带宽的一种手段，例如基于 HBM的片上计算方案，以及AMD提出的概念. 在非 3D 场景下，三星的 AXDIMM和 Marvell 的 Structra-A将处理器直接贴装于商用 DDR DRAM 上。前者在 DIMM 缓冲芯片中集成了计算逻辑，后者则利用 CXL 接口以提升可编程性与系统集成便捷性。

**低延迟互连（Low-latency interconnect）**  
大量文献探讨了低跳数网络拓扑结构，如树形（trees）、蝶形（dragonfly）及高维 Torus 网络。商用片上/近存加速器示例包括：NVIDIA 的 NVLink 与 InfiniBand 交换机——支持交换内减少与多级聚合加速（SHARP，可扩展分层聚合与缩减协议）。类似能力近期亦出现在以太网交换机中。

**软件创新（Software Innovations）**  
除本文聚焦的硬件创新外，还存在大量软硬件协同优化空间，用于改进 LLM 推理效率。例如，Transformer 解码器中的自回归特性是一大根本瓶颈；而新兴算法（如用于图像生成的 Diffusion 模型）若能规避该问题，则可极大简化 AI 推理硬件设计。
# 总结
大语言模型（LLM）推理的重要性与难度日益凸显——其迫切需要降低成本和延迟，因此成为极具吸引力的研究方向。自回归式解码（Autoregressive Decode）本身已对内存和互连延迟构成严峻挑战，而混合专家（MoE）、推理模型、多模态数据、检索增强生成（RAG）以及长输入/输出序列等技术趋势，进一步加剧了这一挑战。

当拥有符合现实的仿真工具时，计算机架构界曾在分支预测、缓存设计等问题上取得过重大突破。鉴于LLM推理的核心瓶颈在于内存与延迟，基于上限分析（Roofline）的性能仿真工具，或许能为多数场景提供一阶性能估算。此外，这类框架还需跟踪内存容量、探索各类对性能至关重要的分片技术，并采用聚焦数据中心容量、系统功耗及碳足迹的新型性能/成本指标，而非传统测量方式。我们期待学术研究者能把握这一机遇，加速人工智能研究进程。

当前AI硬件的核心理念——大画幅芯片、高浮点运算能力（FLOPS）、多HBM堆叠及带宽优化的互连架构——与LLM解码推理的需求并不匹配。虽然众多研究者仍在探索数据中心计算技术，但我们建议将研究重心转向内存与网络层面，沿以下四大方向推进技术改进：**高带宽闪存（HBF）、近存计算（PNM）、3D堆叠以及低延迟互**。此外，针对数据中心容量、系统功耗及碳足迹优化的新型性能/成本指标，相比传统测量方式也带来了新的研究机遇。HBF、PNM、存内计算（PIM）及3D堆叠技术的轻量化版本，同样有望在移动设备端LLM中发挥作用。

这类技术进步将开启协同攻关的序幕，助力实现全球亟需的关键、紧迫创新，为大众提供可负担的人工智能推理服务。
# 相关内容
Zhou, Z., et al., A survey on efficient inference for large language models, 2024, arXiv:2404.14294.

Shilov, A., SanDisk's new High Bandwidth Flash memory, 2025, Tom's Hardware.